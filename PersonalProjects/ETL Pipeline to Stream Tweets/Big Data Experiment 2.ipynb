{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/Users/Asus/Desktop/Backup/All Files\\actual_tweet.csv has been imported.\n",
      "C:/Users/Asus/Desktop/Backup/All Files\\actual_tweet1.csv has been imported.\n",
      "C:/Users/Asus/Desktop/Backup/All Files\\actual_tweet2.csv has been imported.\n"
     ]
    }
   ],
   "source": [
    "#Merge Files\n",
    "import shutil\n",
    "import glob\n",
    "\n",
    "\n",
    "#import csv files from folder\n",
    "path = 'C:/Users/Asus/Desktop/Backup/All Files'\n",
    "allFiles = glob.glob(path + \"/*.csv\")\n",
    "allFiles.sort()  # glob lacks reliable ordering, so impose your own if output order matters\n",
    "with open('C:/Users/Asus/Desktop/Backup/All Files/MergedFile.csv', 'wb') as outfile:\n",
    "    for i, fname in enumerate(allFiles):\n",
    "        with open(fname, 'rb') as infile:\n",
    "            if i != 0:\n",
    "                infile.readline()  # Throw away header on all but first file\n",
    "            # Block copy rest of file from input to output without parsing\n",
    "            shutil.copyfileobj(infile, outfile)\n",
    "            print(fname + \" has been imported.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "#change csv to panda dataframe\n",
    "import pandas as pd\n",
    "import re\n",
    "import emoji \n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "df = pd.read_csv('C:/Users/Asus/Desktop/Backup/All Files/MergedFile.csv')\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To clean all the text data\n",
    "def cleaner(tweet):\n",
    "    tweet = tweet.lower()\n",
    "    tweet = re.sub(\"@[A-Za-z0-9]+\",\"\",tweet) #Remove @ sign\n",
    "    tweet = re.sub(r\"(?:\\@|http?\\://|https?\\://|www)\\S+\", \"\", tweet) #Remove http links\n",
    "    tweet = re.sub(\"#\", \"\", tweet) #Remove hashtags\n",
    "    tweet = re.sub(\"covid19\",\"\",tweet)\n",
    "    tweet = re.sub(\"coronavirus\",\"\",tweet)\n",
    "    tweet = re.sub(\"unitedkingdom\",\"\",tweet)\n",
    "    tweet = re.sub(\"uk\",\"\",tweet)\n",
    "    tweet = re.sub(\"india\",\"\",tweet)\n",
    "    tweet = \" \".join(tweet.split())\n",
    "    tweet = ''.join(c for c in tweet if c not in emoji.UNICODE_EMOJI) #Remove Emojis\n",
    "    return tweet\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_text = df['Text'].map(lambda x:  cleaner(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing Stopwords (It tooks way too long when doing it on the function cleaner thus doing it outside seems more viable)\n",
    "tweet_text = tweet_text.str.split()\n",
    "tweet_text = tweet_text.apply(lambda x: ' '.join([item for item in x if item not in stop]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To count the number of strings for each hashtags/words in a text\n",
    "def wordcount(tweet,substring):\n",
    "    tweet = tweet.lower()\n",
    "    count = tweet.count(substring)\n",
    "    return count\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count for all words/hashtags\n",
    "count1 = df['Text'].apply(wordcount, substring ='covid19')\n",
    "count2 = df['Text'].apply(wordcount, substring ='coronavirus')\n",
    "count3 = df['Text'].apply(wordcount, substring ='unitedkingdom')\n",
    "count4 = df['Text'].apply(wordcount, substring ='uk')\n",
    "count5 = df['Text'].apply(wordcount, substring ='india')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['tweet_text']=tweet_text\n",
    "df1['Covid19_counts']=count1\n",
    "df1['CoronaVirus_counts']=count2\n",
    "df1['UnitedKingdom_counts']=count3\n",
    "df1['UK_counts']=count4\n",
    "df1['India_counts']=count5\n",
    "df1['location']=df['location']\n",
    "df1['Mention_counts']=df['Mention_counts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                             tweet_text  Covid19_counts  \\\n",
       "0               love dogecoin dogecoinarmy dogeday420 ‚Ä¶               0   \n",
       "1     petition: make non-binary legally recognised g...               0   \n",
       "2     believe me. alberta stupidist, nowhere near sh...               0   \n",
       "3     üü£ ip abolish end splitting anti devolution vot...               0   \n",
       "4     _ scotland one favourite places sit. next ligh...               0   \n",
       "...                                                 ...             ...   \n",
       "4525                       loooove kingfisher picture ü§©               0   \n",
       "4526  exist australian british cinema hard think equ...               0   \n",
       "4527  writing exclusively _nw, says figures show nor...               0   \n",
       "4528  finally got vaccine don‚Äôt know everyone worrie...               1   \n",
       "4529  great opportunity get involved bssh overseas w...               0   \n",
       "\n",
       "      CoronaVirus_counts  UnitedKingdom_counts  UK_counts  India_counts  \\\n",
       "0                      0                     0          2             0   \n",
       "1                      0                     0          1             0   \n",
       "2                      0                     0          1             0   \n",
       "3                      0                     0          1             0   \n",
       "4                      0                     0          1             0   \n",
       "...                  ...                   ...        ...           ...   \n",
       "4525                   0                     0          1             0   \n",
       "4526                   0                     0          1             0   \n",
       "4527                   0                     0          1             0   \n",
       "4528                   0                     0          0             0   \n",
       "4529                   0                     0          1             0   \n",
       "\n",
       "                  location  Mention_counts  \n",
       "0          London, England               6  \n",
       "1                  she/her               0  \n",
       "2                Tramworld               1  \n",
       "3     Blaenau Gwentüè¥Û†ÅßÛ†Å¢Û†Å∑Û†Å¨Û†Å≥Û†Åø               0  \n",
       "4                 Cornwall               1  \n",
       "...                    ...             ...  \n",
       "4525                Armley               3  \n",
       "4526        Eastbourne, UK               0  \n",
       "4527   North West, England               4  \n",
       "4528               Belfast               0  \n",
       "4529   North West, England               3  \n",
       "\n",
       "[4530 rows x 8 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tweet_text</th>\n      <th>Covid19_counts</th>\n      <th>CoronaVirus_counts</th>\n      <th>UnitedKingdom_counts</th>\n      <th>UK_counts</th>\n      <th>India_counts</th>\n      <th>location</th>\n      <th>Mention_counts</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>love dogecoin dogecoinarmy dogeday420 ‚Ä¶</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>London, England</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>petition: make non-binary legally recognised g...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>she/her</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>believe me. alberta stupidist, nowhere near sh...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>Tramworld</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>üü£ ip abolish end splitting anti devolution vot...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>Blaenau Gwentüè¥Û†ÅßÛ†Å¢Û†Å∑Û†Å¨Û†Å≥Û†Åø</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>_ scotland one favourite places sit. next ligh...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>Cornwall</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>4525</th>\n      <td>loooove kingfisher picture ü§©</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>Armley</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>4526</th>\n      <td>exist australian british cinema hard think equ...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>Eastbourne, UK</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4527</th>\n      <td>writing exclusively _nw, says figures show nor...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>North West, England</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>4528</th>\n      <td>finally got vaccine don‚Äôt know everyone worrie...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>Belfast</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4529</th>\n      <td>great opportunity get involved bssh overseas w...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>North West, England</td>\n      <td>3</td>\n    </tr>\n  </tbody>\n</table>\n<p>4530 rows √ó 8 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 131
    }
   ],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.to_csv('C:/Users/Asus/Desktop/Backup/All Files/Clean_tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python383jvsc74a57bd031ff804e05c542abfdab620000e5333bc9afc6a565bbee95afa27485d8eec998",
   "display_name": "Python 3.8.3 64-bit (conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}